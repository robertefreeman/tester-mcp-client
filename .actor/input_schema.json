{
  "title": "OpenAI-compatible MCP Client Input",
  "type": "object",
  "schemaVersion": 1,
  "properties": {
    "mcpUrl": {
      "title": "MCP Server URL",
      "type": "string",
      "description": "URL of the Model Context Protocol server to connect to",
      "default": "https://api.apify.com/v2/acts/apify~actors-mcp-server/run-sync-get-dataset-items?token=YOUR_TOKEN",
      "editor": "textfield"
    },
    "systemPrompt": {
      "title": "System Prompt",
      "type": "string",
      "description": "System prompt to use for the AI assistant",
      "default": "You are a helpful AI assistant with access to various tools and services.",
      "editor": "textarea"
    },
    "modelName": {
      "title": "Model Name",
      "type": "string",
      "description": "Name of the language model to use. Supports OpenAI models (gpt-4, gpt-3.5-turbo, etc.) and custom models from OpenAI-compatible endpoints (Ollama, vLLM, local models, etc.). For custom endpoints, you can specify any model name supported by your provider.",
      "default": "gpt-4-turbo",
      "enum": [
        "gpt-4-turbo",
        "gpt-4",
        "gpt-3.5-turbo",
        "llama2",
        "llama2:7b",
        "llama2:13b",
        "llama2:70b",
        "codellama",
        "codellama:7b",
        "codellama:13b",
        "codellama:34b",
        "mistral",
        "mistral:7b",
        "mixtral:8x7b",
        "phi3",
        "phi3:mini",
        "gemma:2b",
        "gemma:7b",
        "custom"
      ],
      "enumTitles": [
        "GPT-4 Turbo (OpenAI)",
        "GPT-4 (OpenAI)",
        "GPT-3.5 Turbo (OpenAI)",
        "Llama 2 (Default)",
        "Llama 2 7B",
        "Llama 2 13B",
        "Llama 2 70B",
        "Code Llama (Default)",
        "Code Llama 7B",
        "Code Llama 13B",
        "Code Llama 34B",
        "Mistral (Default)",
        "Mistral 7B",
        "Mixtral 8x7B",
        "Phi-3 (Default)",
        "Phi-3 Mini",
        "Gemma 2B",
        "Gemma 7B",
        "Custom Model Name"
      ],
      "editor": "select"
    },
    "customModelName": {
      "title": "Custom Model Name",
      "type": "string",
      "description": "Specify a custom model name when 'Custom Model Name' is selected above. This allows you to use any model supported by your OpenAI-compatible endpoint.",
      "editor": "textfield"
    },
    "modelMaxOutputTokens": {
      "title": "Maximum Output Tokens",
      "type": "integer",
      "description": "Maximum number of tokens the model can generate in a single response",
      "default": 4096,
      "minimum": 1,
      "maximum": 32768
    },
    "maxNumberOfToolCallsPerQuery": {
      "title": "Max Tool Calls Per Query",
      "type": "integer",
      "description": "Maximum number of tool calls allowed per user query",
      "default": 10,
      "minimum": 1,
      "maximum": 50
    },
    "toolCallTimeoutSec": {
      "title": "Tool Call Timeout (seconds)",
      "type": "integer",
      "description": "Timeout in seconds for individual tool calls",
      "default": 30,
      "minimum": 5,
      "maximum": 300
    },
    "llmProviderApiKey": {
      "title": "LLM Provider API Key",
      "type": "string",
      "description": "API key for the LLM provider (OpenAI or compatible service). Leave empty to use the platform's built-in billing.",
      "isSecret": true,
      "editor": "textfield"
    },
    "llmProviderBaseUrl": {
      "title": "LLM Provider Base URL",
      "type": "string",
      "description": "Base URL for OpenAI-compatible API endpoints (e.g., http://localhost:11434/v1 for Ollama, or custom vLLM endpoints). Leave empty for OpenAI's official API.",
      "editor": "textfield"
    },
    "headers": {
      "title": "Additional Headers",
      "type": "object",
      "description": "Additional HTTP headers to send with MCP server requests (as JSON object)",
      "default": {},
      "editor": "json"
    }
  },
  "required": ["mcpUrl", "modelName"]
}
