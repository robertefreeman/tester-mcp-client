version: '3.8'

services:
  mcp-client:
    build:
      context: .
      dockerfile: Dockerfile
    
    container_name: mcp-client-app
    
    ports:
      - "5001:5001"
    
    environment:
      # Apify API token for MCP server authentication
      - APIFY_TOKEN=${APIFY_TOKEN}
      
      # OpenAI API key for LLM provider access
      - LLM_PROVIDER_API_KEY=${LLM_PROVIDER_API_KEY}
      
      # Custom OpenAI-compatible endpoint (optional)
      - LLM_PROVIDER_BASE_URL=${LLM_PROVIDER_BASE_URL}
      
      # Model name for custom OpenAI-compatible endpoints (optional)
      # Allows runtime override of model name for Ollama, vLLM, local models, etc.
      - MODEL_NAME=${MODEL_NAME}
      
      - ACTOR_WEB_SERVER_PORT=5001
      - NODE_ENV=production
    
    # Health check for service monitoring
    healthcheck:
      test: ["CMD-SHELL", "node -e \"const http = require('http'); const options = { hostname: 'localhost', port: 5001, path: '/client-info', timeout: 5000 }; const req = http.request(options, (res) => { process.exit(res.statusCode === 200 ? 0 : 1); }); req.on('error', () => process.exit(1)); req.on('timeout', () => process.exit(1)); req.end();\""]
      interval: 30s
      timeout: 10s
      start_period: 5s
      retries: 3
    
    restart: unless-stopped
    
    labels:
      - "com.apify.description=Model Context Protocol Client with OpenAI Integration"
      - "com.apify.version=0.1.0"
      - "com.apify.service=mcp-client"